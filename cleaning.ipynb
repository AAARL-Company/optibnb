{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Pre-Processing Airbnb Data\n",
    "<p> Now that we have a good understanding of what our data looks like, the Airbnb datasets provided need to be cleaned and edited for optimal model usage. This includes performing initial feature selection, imputing missing data, examining collinearity, performing variable transformations, and further pre-processing.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global variables\n",
    "BNB_BLUE = '#007A87'\n",
    "BNB_RED = '#FF5A5F'\n",
    "BNB_DARK_GRAY = '#565A5C'\n",
    "BNB_LIGHT_GRAY = '#CED1CC'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Feature Selection\n",
    "<p>As a baseline, we can start by removing features that we intuitively sense will not impact a listing's price. This includes 18 features:</p>\n",
    "* `scrape_id`: Irrelevant to property data.\n",
    "* `last_scraped`: All within first three days of January, irrelevant to actual property data.\n",
    "* `picture_url`: Irrelevant to property data. \n",
    "* `host_id`: Irrelevant to property data.\n",
    "* `host_name`: Irrelevant to property data; no textual importance.\n",
    "* `host_since`: Irrelevant to property data; no textual importance.\n",
    "* `host_picture_url`: Irrelevant to property data.\n",
    "* `street`: Generic names; location data captured with lower unique count in other geographical features.\n",
    "* `neighbourhood`: The `neighbourhood_cleansed` feature presents the same data in a better format.\n",
    "* `state`: All listings are in the state of NY - this is useless.\n",
    "* `market`: All listings are in the NY market - this is useless.\n",
    "* `country`: All listings are in the USA - this is useless.\n",
    "* `weekly_price`: Function of daily price - should not be a predictor.\n",
    "* `monthly_price`: Function of daily price - should not be a predictor.\n",
    "* `calendar_updated`: Irrelevant to property data.\n",
    "* `calendar_last_scraped`: All within first three days of January, irrelevant to actual property data.\n",
    "* `first_review`: Irrelevant to property data, high unique count.\n",
    "* `last_review`: Irrelevant to property data, high unique count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read in the data \n",
    "listings = pd.read_csv('datasets/listings.csv', delimiter=',')\n",
    "calendar = pd.read_csv('datasets/calendar.csv', delimiter=',', usecols=range(4))\n",
    "\n",
    "# Split into predictor and response\n",
    "y = listings[['price']]\n",
    "\n",
    "# Append price at the end of the listings table\n",
    "del listings['price']\n",
    "listings = listings.join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Features to drop\n",
    "bad_features = ['scrape_id', 'last_scraped', 'picture_url', 'host_picture_url', \n",
    "                'host_id', 'neighbourhood', 'state', 'market', 'country',\n",
    "                'weekly_price', 'monthly_price', 'calendar_last_scraped',\n",
    "                'host_name', 'host_since', 'street', 'calendar_updated',\n",
    "                'first_review', 'last_review']\n",
    "\n",
    "listings.drop(bad_features, axis=1, inplace=True)\n",
    "\n",
    "# Store number of entries and features\n",
    "entries = listings.shape[0]\n",
    "features = listings.shape[1]\n",
    "\n",
    "print 'Number of entries:',entries\n",
    "print 'Number of features:',features\n",
    "listings.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and Imputation\n",
    "<p>We begin by making sure that all quantitative predictors and response variables are float. This will allow us to better deal with categorical data, and NaN entries in the float data. Looking through `zipcode` and `city` we realize that there is a lot of erroneous and incomplete data in these features, . Both of these\n",
    "\n",
    "Before we can on-hot encode, we need to deal with our missing values. For categorical values, we will fill in using the mode because there is no ordering, and for quantitative variables we will use the median (to prevent outliers from skewing the imputation).</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert to floats\n",
    "listings['price'] = listings['price'].apply(lambda s: float(s[1:].replace(',','')))\n",
    "listings['extra_people'] = listings['extra_people'].apply(lambda s: float(s[1:].replace(',','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# List of columns to be converted to floating point\n",
    "to_float = ['id', 'latitude', 'longitude', 'accommodates',\n",
    "            'bathrooms', 'bedrooms', 'beds', 'square_feet',\n",
    "            'guests_included', 'extra_people', 'minimum_nights', \n",
    "            'maximum_nights', 'availability_30', 'availability_60', \n",
    "            'availability_90', 'availability_365', 'number_of_reviews',\n",
    "            'review_scores_rating', 'review_scores_accuracy', \n",
    "            'review_scores_cleanliness', 'review_scores_checkin',\n",
    "            'review_scores_communication', 'review_scores_location',\n",
    "            'review_scores_value', 'host_listing_count']\n",
    "\n",
    "# Converted columns to floating point\n",
    "for feature_name in to_float:\n",
    "    listings[feature_name] = listings[feature_name].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encode_categorical\n",
    "# \n",
    "# Function to label encode categorical variables.\n",
    "#     Input: array (array of values)\n",
    "#     Output: array (array of encoded values)\n",
    "def encode_categorical(array):\n",
    "    if not array.dtype == np.dtype('float64'):\n",
    "        return preprocessing.LabelEncoder().fit_transform(array) \n",
    "    else:\n",
    "        return array\n",
    "\n",
    "new_x = x.apply(encode_categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Emptiness and Erroneous Data\n",
    "<p>Before imputing missing values, we should examine the percentage of values that are missing from each feature. Imputing data for a feature with too much missing data can bias the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Returns percent of missing data in column\n",
    "def percent_empty(df):\n",
    "    \n",
    "    bools = df.isnull().tolist()\n",
    "    percent_empty = float(bools.count(True)) / float(len(bools))\n",
    "    \n",
    "    return percent_empty\n",
    "\n",
    "# Store emptiness for all features\n",
    "emptiness = []\n",
    "\n",
    "# Get emptiness for all features\n",
    "for i in range(0, listings.shape[1]):\n",
    "    emptiness.append(round(percent_empty(listings.iloc[:,i]), 2))\n",
    "    \n",
    "empty_dict = dict(zip(listings.columns.values.tolist(), emptiness))\n",
    "\n",
    "# Plot emptiness graph\n",
    "empty = pd.DataFrame.from_dict(empty_dict, orient = 'index').sort_values(by=0)\n",
    "ax = empty.plot(kind = 'bar', color='#E35A5C', figsize = (16, 5))\n",
    "ax.set_xlabel('Predictor')\n",
    "ax.set_ylabel('Percent Empty / NaN')\n",
    "ax.set_title('Feature Emptiness')\n",
    "ax.legend_.remove()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The percent emptiness graph shows that `square_feet` is over 90% empty. This is too empty for imputation, so we remove this feature. We also remove entries (rows) that have faulty data such as:\n",
    "\n",
    "* There are 0 bedrooms\n",
    "* There are 0 bathrooms\n",
    "* There are 0 beds\n",
    "* The price is \\$0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listings.drop('square_feet', axis=1, inplace=True)\n",
    "\n",
    "# Delete bad entries\n",
    "listings = listings[listings.bedrooms != 0]\n",
    "listings = listings[listings.beds != 0]\n",
    "listings = listings[listings.bathrooms != 0]\n",
    "listings = listings[listings.price != 0]\n",
    "\n",
    "print 'Number of entries removed: ', entries - listings.shape[0]\n",
    "entries = listings.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trimming Neighborhood Entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>When we explored our data we saw that geography was very important to pricing, especially on Manhattan. The `neighbourhood_cleansed` feature could therefore be important. Looking at the distribution below we notice it is heavily left-skewed.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get number of listings in neighborhoods\n",
    "nb_counts = Counter(listings.neighbourhood_cleansed)\n",
    "tdf = pd.DataFrame.from_dict(nb_counts, orient='index').sort_values(by=0)\n",
    "\n",
    "# Plot number of listings in each neighborhood\n",
    "ax = tdf.plot(kind='bar', figsize = (50,10), color = BNB_BLUE, alpha = 0.85)\n",
    "ax.set_title(\"Neighborhoods by Number of Listings\")\n",
    "ax.set_xlabel(\"Neighborhood\")\n",
    "ax.set_ylabel(\"# of Listings\")\n",
    "plt.show()\n",
    "\n",
    "print \"Number of Neighborhoods:\", len(nb_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We see that the majority of the neighborhoods have less than 100 listings. We currently have 186 neighborhoods - all of these categorical predictors when one-hot encoded will not be significant, so we will only keep neighborhoods with more than 100 listings. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Delete neighborhoods with less than 100 entries\n",
    "for i in nb_counts.keys():\n",
    "    if nb_counts[i] < 100:\n",
    "        del nb_counts[i]\n",
    "\n",
    "tdf = pd.DataFrame.from_dict(nb_counts, orient='index').sort_values(by=0)\n",
    "ax = tdf.plot(kind='bar', figsize = (22,4), color = BNB_BLUE, alpha = 0.85)\n",
    "ax.set_title(\"Neighborhoods by House # (Top 48)\")\n",
    "ax.set_xlabel(\"Neighborhood\")\n",
    "ax.set_ylabel(\"# of Listings\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listings.to_csv(path_or_buf='../clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
